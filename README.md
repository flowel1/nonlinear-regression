# Levenberg-Marquardt algorithm for nonlinear regression

## Nonlinear regression: generic statement
Assume that we have N observations

<a href="https://www.codecogs.com/eqnedit.php?latex=(\underline{x}_1,&space;y_1),&space;...,&space;(\underline{x}_N,&space;y_N)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\underline{x}_1,&space;y_1),&space;...,&space;(\underline{x}_N,&space;y_N)" title="(\underline{x}_1, y_1), ..., (\underline{x}_N, y_N)" /></a>

where <a href="https://www.codecogs.com/eqnedit.php?latex=\underline{x}_i&space;\in&space;\mathbb{R}^n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\underline{x}_i&space;\in&space;\mathbb{R}^n" title="\underline{x}_i \in \mathbb{R}^n" /></a> are observable predictors and <a href="https://www.codecogs.com/eqnedit.php?latex=y_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i" title="y_i" /></a> are target real variables (i.e., the variables that must be predicted).

We would like to estimate a nonlinear model of the form

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f(\underline{x};\underline{\theta})&space;&plus;&space;\epsilon" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f(\underline{x};\underline{\theta})&space;&plus;&space;\epsilon" title="y = f(\underline{x};\underline{\theta}) + \epsilon" /></a>

where <ins>&theta;</ins>  is a vector of _k_ unknown real parameters, _f_ is a known function nonlinear in <ins>&theta;</ins> and <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon&space;\sim&space;N(0,&space;\sigma^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon&space;\sim&space;N(0,&space;\sigma^2)" title="\epsilon \sim N(0, \sigma^2)" /></a> for some positive value of &sigma;.

Setting

<a href="https://www.codecogs.com/eqnedit.php?latex=X&space;=\begin{bmatrix}&space;\underline{x}_1&space;\\&space;...&space;\\&space;\underline{x}_N&space;\\&space;\end{bmatrix}&space;\in&space;\mathbb{R}^{N&space;\times&space;n},&space;\hspace{10}&space;\underline{y}&space;=&space;\begin{bmatrix}&space;y_1&space;\\&space;...&space;\\&space;y_N&space;\end{bmatrix}&space;\in&space;\mathbb{R}^N,&space;\hspace{10}&space;\underline{\epsilon}&space;=&space;\begin{bmatrix}&space;\epsilon_1&space;\\&space;...&space;\\&space;\epsilon_N&space;\end{bmatrix}&space;\in&space;\mathbb{R}^N" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X&space;=\begin{bmatrix}&space;\underline{x}_1&space;\\&space;...&space;\\&space;\underline{x}_N&space;\\&space;\end{bmatrix}&space;\in&space;\mathbb{R}^{N&space;\times&space;n},&space;\hspace{10}&space;\underline{y}&space;=&space;\begin{bmatrix}&space;y_1&space;\\&space;...&space;\\&space;y_N&space;\end{bmatrix}&space;\in&space;\mathbb{R}^N,&space;\hspace{10}&space;\underline{\epsilon}&space;=&space;\begin{bmatrix}&space;\epsilon_1&space;\\&space;...&space;\\&space;\epsilon_N&space;\end{bmatrix}&space;\in&space;\mathbb{R}^N" title="X =\begin{bmatrix} \underline{x}_1 \\ ... \\ \underline{x}_N \\ \end{bmatrix} \in \mathbb{R}^{N \times n}, \hspace{10} \underline{y} = \begin{bmatrix} y_1 \\ ... \\ y_N \end{bmatrix} \in \mathbb{R}^N, \hspace{10} \underline{\epsilon} = \begin{bmatrix} \epsilon_1 \\ ... \\ \epsilon_N \end{bmatrix} \in \mathbb{R}^N" /></a>

and assuming that the N observations are independent, the log-likelihood of the model given our _N_ observations is

<a href="https://www.codecogs.com/eqnedit.php?latex=L(\underline{\theta}&space;|&space;X,&space;\underline{y})&space;=&space;\prod_{i&space;=&space;1}^N&space;\frac{1}{\sigma\sqrt{2\pi}}\exp&space;\left&space;\{&space;-&space;\left(&space;\frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma}&space;\right)&space;^&space;2&space;\right&space;\}&space;\Rightarrow&space;\log&space;L(\underline{\theta}&space;|&space;X,&space;\underline{y})&space;=&space;-\frac{1}{(2\pi\sigma^2)^\frac{N}{2}}&space;\sum_{i=1}^N&space;\left(&space;\frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma}&space;\right)&space;^&space;2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L(\underline{\theta}&space;|&space;X,&space;\underline{y})&space;=&space;\prod_{i&space;=&space;1}^N&space;\frac{1}{\sigma\sqrt{2\pi}}\exp&space;\left&space;\{&space;-&space;\left(&space;\frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma}&space;\right)&space;^&space;2&space;\right&space;\}&space;\Rightarrow&space;\log&space;L(\underline{\theta}&space;|&space;X,&space;\underline{y})&space;=&space;-\frac{1}{(2\pi\sigma^2)^\frac{N}{2}}&space;\sum_{i=1}^N&space;\left(&space;\frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma}&space;\right)&space;^&space;2" title="L(\underline{\theta} | X, \underline{y}) = \prod_{i = 1}^N \frac{1}{\sigma\sqrt{2\pi}}\exp \left \{ - \left( \frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma} \right) ^ 2 \right \} \Rightarrow \log L(\underline{\theta} | X, \underline{y}) = -\frac{1}{(2\pi\sigma^2)^\frac{N}{2}} \sum_{i=1}^N \left( \frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma} \right) ^ 2" /></a>

We estimate the model parameters by maximizing the log-likelihood with respect to <ins>&theta;</ins>. This is equivalent to minimizing the following objective function (sum of squared residuals):

<a href="https://www.codecogs.com/eqnedit.php?latex=\text{Obj}(\underline{\theta})&space;=&space;\left&space;\|&space;\underline{y}&space;-&space;f(X,&space;\underline{\theta})&space;\right&space;\|&space;^&space;2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\text{Obj}(\underline{\theta})&space;=&space;\left&space;\|&space;\underline{y}&space;-&space;f(X,&space;\underline{\theta})&space;\right&space;\|&space;^&space;2" title="\text{Obj}(\underline{\theta}) = \left \| \underline{y} - f(X, \underline{\theta}) \right \| ^ 2" /></a>

## The Levenberg-Marquardt algorithm

Start from <a href="https://www.codecogs.com/eqnedit.php?latex=\underline{\theta}^{(0)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\underline{\theta}^{(0)}" title="\underline{\theta}^{(0)}" /></a> and approximate the objective function around <a href="https://www.codecogs.com/eqnedit.php?latex=\underline{\theta}^{(0)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\underline{\theta}^{(0)}" title="\underline{\theta}^{(0)}" /></a> with the following quadratic function:

<a href="https://www.codecogs.com/eqnedit.php?latex=\phi(\underline{\delta})&space;=&space;\left&space;\|&space;\underline{y}&space;-&space;f(X,&space;\underline{\theta}^{(0)})&space;-&space;J_{\underline{\theta}}f(X,&space;\underline{\theta})_{|_{\underline{\theta}&space;=&space;\underline{\theta}^{(0)}}}&space;\cdot&space;\underline{\delta}&space;\right&space;\|&space;^2&space;\sim&space;\text{Obj}(\underline{\theta}^{(0)}&space;&plus;&space;\underline{\delta})" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\phi(\underline{\delta})&space;=&space;\left&space;\|&space;\underline{y}&space;-&space;f(X,&space;\underline{\theta}^{(0)})&space;-&space;J_{\underline{\theta}}f(X,&space;\underline{\theta})_{|_{\underline{\theta}&space;=&space;\underline{\theta}^{(0)}}}&space;\cdot&space;\underline{\delta}&space;\right&space;\|&space;^2&space;\sim&space;\text{Obj}(\underline{\theta}^{(0)}&space;&plus;&space;\underline{\delta})" title="\phi(\underline{\delta}) = \left \| \underline{y} - f(X, \underline{\theta}^{(0)}) - J_{\underline{\theta}}f(X, \underline{\theta})_{|_{\underline{\theta} = \underline{\theta}^{(0)}}} \cdot \underline{\delta} \right \| ^2 \sim \text{Obj}(\underline{\theta}^{(0)} + \underline{\delta})" /></a>

Thanks to the objective function's special form, we can calculate a local quadratic approximation by taking the first order expansion of _f_ instead of the second-order expansion of the objective function itself.

Defining for simplicity

<a href="https://www.codecogs.com/eqnedit.php?latex=J^{(0)}&space;:=&space;J_{\underline{\theta}}f(X,&space;\underline{\theta})_{|_{\underline{\theta}&space;=&space;\underline{\theta}^{(0)}}}&space;\in&space;\mathbb{R}^{N&space;\times&space;k},&space;\hspace{10}&space;\underline{\epsilon}^{(0)}&space;:=&space;\underline{y}&space;-&space;f(X,&space;\underline{\theta}^{(0)})&space;\in&space;\mathbb{R}^N" target="_blank"><img src="https://latex.codecogs.com/gif.latex?J^{(0)}&space;:=&space;J_{\underline{\theta}}f(X,&space;\underline{\theta})_{|_{\underline{\theta}&space;=&space;\underline{\theta}^{(0)}}}&space;\in&space;\mathbb{R}^{N&space;\times&space;k},&space;\hspace{10}&space;\underline{\epsilon}^{(0)}&space;:=&space;\underline{y}&space;-&space;f(X,&space;\underline{\theta}^{(0)})&space;\in&space;\mathbb{R}^N" title="J^{(0)} := J_{\underline{\theta}}f(X, \underline{\theta})_{|_{\underline{\theta} = \underline{\theta}^{(0)}}} \in \mathbb{R}^{N \times k}, \hspace{10} \underline{\epsilon}^{(0)} := \underline{y} - f(X, \underline{\theta}^{(0)}) \in \mathbb{R}^N" /></a>,

we have that this quadratic approximation reaches its minimum when

<a href="https://www.codecogs.com/eqnedit.php?latex=\nabla_{\underline{\delta}}&space;\hspace{2}&space;\phi(\underline{\delta})&space;=&space;-2&space;\cdot&space;J^{(0)}^T&space;\cdot&space;(\underline{\epsilon}^{(0)}&space;-&space;J^{(0)}&space;\cdot&space;\underline{\delta})&space;=&space;\underline{0}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\nabla_{\underline{\delta}}&space;\hspace{2}&space;\phi(\underline{\delta})&space;=&space;-2&space;\cdot&space;J^{(0)}^T&space;\cdot&space;(\underline{\epsilon}^{(0)}&space;-&space;J^{(0)}&space;\cdot&space;\underline{\delta})&space;=&space;\underline{0}" title="\nabla_{\underline{\delta}} \hspace{2} \phi(\underline{\delta}) = -2 \cdot J^{(0)}^T \cdot (\underline{\epsilon}^{(0)} - J^{(0)} \cdot \underline{\delta}) = \underline{0}" /></a>

which is satisfied when the displacement <ins>&delta;</ins> solves the following linear system:

<a href="https://www.codecogs.com/eqnedit.php?latex=(J^{(0)}^T&space;\cdot&space;J^{(0)})&space;\cdot&space;\underline{\delta}&space;=&space;J^{(0)}&space;\cdot&space;\underline{\epsilon}^{(0)}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(J^{(0)}^T&space;\cdot&space;J^{(0)})&space;\cdot&space;\underline{\delta}&space;=&space;J^{(0)}&space;\cdot&space;\underline{\epsilon}^{(0)}" title="(J^{(0)}^T \cdot J^{(0)}) \cdot \underline{\delta} = J^{(0)} \cdot \underline{\epsilon}^{(0)}" /></a>
