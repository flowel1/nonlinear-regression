# Levenberg-Marquardt algorithm for nonlinear regression

Assume that we have N observations

<a href="https://www.codecogs.com/eqnedit.php?latex=(\underline{x}_1,&space;y_1),&space;...,&space;(\underline{x}_N,&space;y_N)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?(\underline{x}_1,&space;y_1),&space;...,&space;(\underline{x}_N,&space;y_N)" title="(\underline{x}_1, y_1), ..., (\underline{x}_N, y_N)" /></a>

where <a href="https://www.codecogs.com/eqnedit.php?latex=\underline{x}_i&space;\in&space;\mathbb{R}^n" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\underline{x}_i&space;\in&space;\mathbb{R}^n" title="\underline{x}_i \in \mathbb{R}^n" /></a> are observable predictors and <a href="https://www.codecogs.com/eqnedit.php?latex=y_i" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y_i" title="y_i" /></a> are target real variables (i.e., the variables that must be predicted).

We would like to estimate a nonlinear model of the form

<a href="https://www.codecogs.com/eqnedit.php?latex=y&space;=&space;f(\underline{x};\underline{\theta})&space;&plus;&space;\epsilon" target="_blank"><img src="https://latex.codecogs.com/gif.latex?y&space;=&space;f(\underline{x};\underline{\theta})&space;&plus;&space;\epsilon" title="y = f(\underline{x};\underline{\theta}) + \epsilon" /></a>

where <ins>&theta;</ins>  is a vector of _k_ unknown real parameters, _f_ is a known function nonlinear in <ins>&theta;</ins> and <a href="https://www.codecogs.com/eqnedit.php?latex=\epsilon&space;\sim&space;N(0,&space;\sigma^2)" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\epsilon&space;\sim&space;N(0,&space;\sigma^2)" title="\epsilon \sim N(0, \sigma^2)" /></a> for some positive value of &sigma;.

Setting

<a href="https://www.codecogs.com/eqnedit.php?latex=X&space;=\begin{bmatrix}&space;\underline{x}_1&space;\\&space;...&space;\\&space;\underline{x}_N&space;\\&space;\end{bmatrix}&space;\in&space;\mathbb{R}^{N&space;\times&space;n},&space;\hspace{10}&space;\underline{y}&space;=&space;\begin{bmatrix}&space;y_1&space;\\&space;...&space;\\&space;y_N&space;\end{bmatrix}&space;\in&space;\mathbb{R}^N,&space;\hspace{10}&space;\underline{\epsilon}&space;=&space;\begin{bmatrix}&space;\epsilon_1&space;\\&space;...&space;\\&space;\epsilon_N&space;\end{bmatrix}&space;\in&space;\mathbb{R}^N" target="_blank"><img src="https://latex.codecogs.com/gif.latex?X&space;=\begin{bmatrix}&space;\underline{x}_1&space;\\&space;...&space;\\&space;\underline{x}_N&space;\\&space;\end{bmatrix}&space;\in&space;\mathbb{R}^{N&space;\times&space;n},&space;\hspace{10}&space;\underline{y}&space;=&space;\begin{bmatrix}&space;y_1&space;\\&space;...&space;\\&space;y_N&space;\end{bmatrix}&space;\in&space;\mathbb{R}^N,&space;\hspace{10}&space;\underline{\epsilon}&space;=&space;\begin{bmatrix}&space;\epsilon_1&space;\\&space;...&space;\\&space;\epsilon_N&space;\end{bmatrix}&space;\in&space;\mathbb{R}^N" title="X =\begin{bmatrix} \underline{x}_1 \\ ... \\ \underline{x}_N \\ \end{bmatrix} \in \mathbb{R}^{N \times n}, \hspace{10} \underline{y} = \begin{bmatrix} y_1 \\ ... \\ y_N \end{bmatrix} \in \mathbb{R}^N, \hspace{10} \underline{\epsilon} = \begin{bmatrix} \epsilon_1 \\ ... \\ \epsilon_N \end{bmatrix} \in \mathbb{R}^N" /></a>

and assuming that the N observations are independent, the log-likelihood of the model given our _N_ observations is

<a href="https://www.codecogs.com/eqnedit.php?latex=L(\underline{\theta}&space;|&space;X,&space;\underline{y})&space;=&space;\prod_{i&space;=&space;1}^N&space;\frac{1}{\sigma\sqrt{2\pi}}\exp&space;\left&space;\{&space;-&space;\left(&space;\frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma}&space;\right)&space;^&space;2&space;\right&space;\}&space;\Rightarrow&space;\log&space;L(\underline{\theta}&space;|&space;X,&space;\underline{y})&space;=&space;-\frac{1}{(2\pi\sigma^2)^\frac{N}{2}}&space;\sum_{i=1}^N&space;\left(&space;\frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma}&space;\right)&space;^&space;2" target="_blank"><img src="https://latex.codecogs.com/gif.latex?L(\underline{\theta}&space;|&space;X,&space;\underline{y})&space;=&space;\prod_{i&space;=&space;1}^N&space;\frac{1}{\sigma\sqrt{2\pi}}\exp&space;\left&space;\{&space;-&space;\left(&space;\frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma}&space;\right)&space;^&space;2&space;\right&space;\}&space;\Rightarrow&space;\log&space;L(\underline{\theta}&space;|&space;X,&space;\underline{y})&space;=&space;-\frac{1}{(2\pi\sigma^2)^\frac{N}{2}}&space;\sum_{i=1}^N&space;\left(&space;\frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma}&space;\right)&space;^&space;2" title="L(\underline{\theta} | X, \underline{y}) = \prod_{i = 1}^N \frac{1}{\sigma\sqrt{2\pi}}\exp \left \{ - \left( \frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma} \right) ^ 2 \right \} \Rightarrow \log L(\underline{\theta} | X, \underline{y}) = -\frac{1}{(2\pi\sigma^2)^\frac{N}{2}} \sum_{i=1}^N \left( \frac{y_i-f(\underline{x}_i;\underline{\theta})}{\sigma} \right) ^ 2" /></a>

We estimate the model parameters by maximizing the log-likelihood with respect to <ins>&theta;</ins>. This is equivalent to minimizing the mean squared error.
